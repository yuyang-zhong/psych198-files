{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psych 198: Reproducibility DeCal (Spring 2021)\n",
    "\n",
    "## Demo/Lab 3: Error Metrics\n",
    "\n",
    "In this demo/lab, we will go through some of the error metrics used in data science. Code adopted from Data 144.\n",
    "\n",
    "Note that this notebook is Python-based, so if you have any questions regarding the syntax of the code, please feel free to reach out or sign up for an office hour slot. \n",
    "\n",
    "Author: Yuyang Zhong (2021). This work is licensed under a [Creative Commons BY-NC-SA 4.0 International\n",
    "License][cc-by]. \n",
    "\n",
    "![CC BY-NC-SA 4.0][cc-by-shield]\n",
    "\n",
    "[cc-by]: http://creativecommons.org/licenses/by/4.0/\n",
    "[cc-by-shield]: https://img.shields.io/badge/license-CC--BY--NC--SA%204.0-blue\n",
    "\n",
    "#### Note on using Jupyter Notebooks \n",
    "Enter code into a code cell, then press SHIFT+Enter to run that cell. The output of the code should be shown right underneath the cell you just run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will be using a dataset about the passengers on the Titanic, and try to predict whether someone will survive or not. Run the cell below to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values for df_train\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_age_by_pclass(df):\n",
    "    group_mean = df.groupby('Pclass').mean()['Age']\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if np.isnan(row['Age']):\n",
    "            df.at[index,'Age'] = group_mean[row['Pclass']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing values for age column with the mean of the passenger class group\n",
    "fill_age_by_pclass(df_train)\n",
    "fill_age_by_pclass(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating missing embarking port based on plot above:\n",
    "\n",
    "* For anyone in 1st class, sample from [S, S, C, C, C, Q]\n",
    "* For anyone in 2nd class, sample from [S, S, S, C, Q]\n",
    "* For anyone in 3rd class, sample from [S, S, C, Q, Q, Q, Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_embark_by_pclass(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Embarked'] != row['Embarked']:\n",
    "            if row['Pclass'] == 1:\n",
    "                df.at[index,'Embarked'] = np.random.choice(['S', 'S', 'C', 'C', 'C', 'Q'])\n",
    "            elif row['Pclass'] == 2:\n",
    "                df.at[index,'Embarked'] = np.random.choice(['S', 'S', 'S', 'C', 'Q'])\n",
    "            else:\n",
    "                df.at[index,'Embarked'] = np.random.choice(['S', 'S', 'C', 'Q', 'Q', 'Q', 'Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing values for embarked column with a approximate probabilistic sampling by class\n",
    "fill_embark_by_pclass(df_train)\n",
    "fill_embark_by_pclass(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace null values of the cabin column with 0 and non null with 1\n",
    "df_train['Cabin'].loc[~df_train['Cabin'].isnull()] = 1\n",
    "df_train['Cabin'].loc[df_train['Cabin'].isnull()] = 0\n",
    "\n",
    "df_test['Cabin'].loc[~df_test['Cabin'].isnull()] = 1\n",
    "df_test['Cabin'].loc[df_test['Cabin'].isnull()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values in df_test\n",
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Salutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salutations(series):\n",
    "    suffix_train = []\n",
    "    for elem in series:\n",
    "        suff = re.findall('[A-Z]{1}[a-z]{1,}[.]', elem)\n",
    "        suffix_train.append(suff[0])\n",
    "    return suffix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the title from the name\n",
    "df_train['Title'] = extract_salutations(df_train['Name'])\n",
    "df_test['Title'] = extract_salutations(df_test['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace some titles with Mr. and Ms. in df_train so it has same # of unique title as df_test\n",
    "df_train['Title'] = df_train['Title'].replace(['Major.', 'Sir.', 'Capt.', 'Jonkheer.'], 'Mr.').replace(\n",
    "    ['Mlle.', 'Mme.', 'Lady.', 'Countess.'], 'Miss.')\n",
    "df_train['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Num Relatives'] = df_train['SibSp'] + df_train['Parch']\n",
    "df_test['Num Relatives'] = df_test['SibSp'] + df_test['Parch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup final dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feats = ['Pclass', 'Sex', 'Age', 'Embarked', 'Num Relatives', 'Title']\n",
    "\n",
    "x_train = df_train[x_feats]\n",
    "y_train = df_train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "x_train = preprocessing.scale(pd.get_dummies(x_train))\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_model = LogisticRegression(penalty='l1', solver='liblinear', \n",
    "                               max_iter=50, random_state=42)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "log_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(log_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also report the accuracy, recall, precision, and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, log_model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is also known as the true positive rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train, log_model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is also known as the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train, log_model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_score(y_train, log_model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, log_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, log_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, log_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try this with another model/classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
