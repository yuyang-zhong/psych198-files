{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psych 198: Reproducibility DeCal (Spring 2021)\n",
    "\n",
    "## Demo/Lab 2: Cross Validation\n",
    "\n",
    "In this demo/lab, we will go through the a typical process for cross validating a linear model. Code adopted from Psych 102.\n",
    "\n",
    "Author: Yuyang Zhong (2020). This work is licensed under a [Creative Commons BY-NC-SA 4.0 International\n",
    "License][cc-by]. \n",
    "\n",
    "![CC BY-NC-SA 4.0][cc-by-shield]\n",
    "\n",
    "[cc-by]: http://creativecommons.org/licenses/by/4.0/\n",
    "[cc-by-shield]: https://img.shields.io/badge/license-CC--BY--NC--SA%204.0-blue\n",
    "\n",
    "#### Note on using Jupyter Notebooks \n",
    "Enter code into a code cell, then press SHIFT+Enter to run that cell. The output of the code should be shown right underneath the cell you just run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('psychTools')\n",
    "install.packages('caTools')\n",
    "\n",
    "library(car)\n",
    "library(psychTools)\n",
    "library(caTools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "We will be using the The Motivational State Questionnaire (MSQ). If you are interested in looking more into this dataset, run the code `?msq`.\n",
    "\n",
    "Let's take a look at this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(msq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel <- complete.cases(msq[,'idle']) & complete.cases(msq[,'inspired']) & complete.cases(msq[, 'intense']) &\n",
    "       complete.cases(msq[,'interested']) & complete.cases(msq[,'irritable']) & \n",
    "       complete.cases(msq[,'satisfied']) & complete.cases(msq[,'scared']) & complete.cases(msq[,'sleepy']) &\n",
    "       complete.cases(msq[,'strong']) & complete.cases(msq[,'sociable']) & complete.cases(msq[,'happy'])\n",
    "new_msq <- msq[sel, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model 1\n",
    "\n",
    "Using multiple linear regression, fit a general linear model to predict `happy` using all affects that start with i : `idle` + `inspired` + `intense` + `interested` + `irritable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_mod <- lm(happy ~ idle + inspired + intense + interested + irritable, data=new_msq)\n",
    "summary(i_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model 2\n",
    "\n",
    "In this model, we will combine all these predictors and use: `idle`, `inspired`, `intense`, `interested`, `irritable`, `satisfied`, `scared`, `sleepy`, `strong` and `sociable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_mod <- lm(happy ~ idle + inspired + intense + interested + irritable +\n",
    "             satisfied + scared + sleepy + strong + sociable, data = new_msq)\n",
    "summary(is_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference in MSE\n",
    "\n",
    "This is another measure we will take a look at: did the new model reduce error/noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse1 <- mean(i_mod$residuals^2)\n",
    "mse2 <- mean(is_mod$residuals^2)\n",
    "\n",
    "mse1-mse2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to first partition the data into a training set and a test set. We will do a 75-25 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require(caTools)\n",
    "set.seed(101)\n",
    "\n",
    "sample <- sample.split(new_msq[,1], SplitRatio = .75)\n",
    "train <- subset(new_msq, sample == TRUE)\n",
    "test <- subset(new_msq, sample == FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrow(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run our linear model through cross validation to see if the statistics we got is representative when the data is shuffled. We will do a 20-fold CV.\n",
    "\n",
    "Again, **ONLY USE TRAINING DATA HERE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.folds <- 20\n",
    "folds <- cut(seq(1,nrow(train)),breaks=n.folds,labels=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create some empty arrays to collect the models and the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE.imod <- array(data=0, dim = n.folds)    # Space for MSE\n",
    "MSE.ismod <- array(data=0, dim = n.folds)\n",
    "\n",
    "DE.i.is <- array(data=0, dim = n.folds)     # Space for differences in MSE\n",
    "R2.i.is <- array(data=0, dim = n.folds)     # Space for R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a for-loop to help us run cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 1:n.folds){\n",
    "    \n",
    "    #Segement your data by fold using the which() function \n",
    "    validateIdx <- which(folds==i,arr.ind=TRUE)\n",
    "    validateData <- train[validateIdx, ]\n",
    "    trainData <- train[-validateIdx, ]\n",
    "\n",
    "    # Fit the threes models\n",
    "    i_lmod <- lm(happy ~ idle + inspired + intense + interested + irritable, \n",
    "                 data=trainData)\n",
    "    is_lmod <- lm(happy ~ idle + inspired + intense + interested + irritable + \n",
    "                  satisfied + scared + sleepy + strong + sociable,\n",
    "                  data = trainData)\n",
    "    \n",
    "    # Get predictions with validation data\n",
    "    i_preds <- predict(i_lmod, validateData)\n",
    "    is_preds <- predict(is_lmod, validateData)\n",
    "    \n",
    "    # Calculate the mean square errors.\n",
    "    y <- validateData$happy\n",
    "    MSE.imod[i] <- mean((y - i_preds)^2)\n",
    "    MSE.ismod[i] <- mean((y - is_preds)^2)\n",
    "\n",
    "    # Differences in error: (note - this can also be calculated outside the loop)\n",
    "    DE.i.is[i] <- MSE.imod[i] - MSE.ismod[i]\n",
    "    \n",
    "    # Calculate R2 for the nested models. (this can also be done outside the loop)\n",
    "    R2.i.is[i] <- 1 - (MSE.ismod[i]/MSE.imod[i])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then look at the averaged results from our cross validation, and compare it with our ANOVA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paste0(\"The average MSE for the model 1 was \", round(mean(MSE.imod),3))\n",
    "paste0(\"The average MSE for the model 2 was \", round(mean(MSE.ismod),3))\n",
    "paste0(\"The average R-squared for comparison between model 1 and 2 was \",\n",
    "       round(mean(R2.i.is),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot and see what the average differences in MSE are, and where our original model stand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_i_is <- mean(DE.i.is)\n",
    "se_i_is <- sd(DE.i.is)\n",
    "c(mean_i_is-2*se_i_is, mean_i_is+2*se_i_is)\n",
    "hist(DE.i.is)\n",
    "abline(v=c(mean_i_is-2*se_i_is, mean_i_is+2*se_i_is), col=\"red\")\n",
    "\n",
    "abline(v=c(mse1-mse2), col=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "Now let's run our 2 models to predict our test data, and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_preds_test <- predict(i_lmod, test)\n",
    "is_preds_test <- predict(is_lmod, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y <- test$happy\n",
    "mse1_test <- mean((y - i_preds_test)^2)\n",
    "mse2_test <- mean((y - is_preds_test)^2)\n",
    "\n",
    "mse1_test - mse2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually pretty close to what we had with our training set original models, as well as our cross validation results. We can plot this onto our histogram as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_i_is <- mean(DE.i.is)\n",
    "se_i_is <- sd(DE.i.is)\n",
    "c(mean_i_is-2*se_i_is, mean_i_is+2*se_i_is)\n",
    "hist(DE.i.is)\n",
    "abline(v=c(mean_i_is-2*se_i_is, mean_i_is+2*se_i_is), col=\"red\")\n",
    "\n",
    "abline(v=c(mse1-mse2), col=\"blue\")\n",
    "abline(v=c(mse1_test-mse2_test), col=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "There is another statistic in the model we have computed and stored, the $R^2$ of the nested model. Repeat this whole process and see if we can get similar results for $R^2$ as the differences in MSE we have calculated here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
